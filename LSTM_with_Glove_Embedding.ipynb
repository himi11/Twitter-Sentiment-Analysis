{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM with Glove Embedding.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxS5hmyWsin0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "e91c803d-aae9-4419-e3db-2a9b04375e02"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04ZhoczVpONC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "outputId": "df4f59f2-e974-42e4-f6c2-cf1a70ca93ce"
      },
      "source": [
        "pip install emoji"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting emoji\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/8d/521be7f0091fe0f2ae690cc044faf43e3445e0ff33c574eae752dd7e39fa/emoji-0.5.4.tar.gz (43kB)\n",
            "\r\u001b[K     |███████▌                        | 10kB 19.3MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 20kB 3.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 30kB 4.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 40kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 2.7MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-0.5.4-cp36-none-any.whl size=42176 sha256=a7d6d9fab094ef14eef3a8ada5f9e81b2b8540c14ddd6e0e4a0a95f28a0e1f31\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/a9/0a/4f8e8cce8074232aba240caca3fade315bb49fac68808d1a9c\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-0.5.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6p-yTUw82th",
        "colab_type": "text"
      },
      "source": [
        "**LSTM with Glove word embedding I got 67.5% accuracy on Test data.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5IzKn3HBTR-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "4fc8f06d-e662-4631-e115-e8ec7378a0b2"
      },
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import emoji\n",
        "import itertools\n",
        "from keras.layers import Conv1D, MaxPooling1D, Dropout, Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "data = pd.read_csv('/content/drive/My Drive/train.txt')\n",
        "train_dataset = pd.DataFrame(data, columns= ['tweet_id','sentiment','tweet_text'])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uYkkyvpuNPEB",
        "colab": {}
      },
      "source": [
        "def load_dict_contractions():\n",
        "    \n",
        "    return {\n",
        "        \"ain't\":\"is not\",\n",
        "        \"lmao\" :\"laughing out loud\",\n",
        "        \"lol\":\"laughing out loud\",\n",
        "        \"aint\":\"is not\",\n",
        "        \"ain\":\"is not\",\n",
        "        \"amn't\":\"am not\",\n",
        "        \"amnt\":\"am not\",\n",
        "        \"amn\":\"am not\",\n",
        "        \"aren't\":\"are not\",\n",
        "        \"arent\":\"are not\",\n",
        "        \"aren\":\"are not\",\n",
        "        \"can't\":\"cannot\",\n",
        "        \"cant\":\"cannot\",\n",
        "        \"'cause\":\"because\",\n",
        "        \"couldn't\":\"could not\",\n",
        "        \"couldnt\":\"could not\",\n",
        "        \"couldn\":\"could not\",\n",
        "        \"couldn't've\":\"could not have\",\n",
        "        \"couldnt've\":\"could not have\",\n",
        "        \"couldn've\":\"could not have\",        \n",
        "        \"could've\":\"could have\",\n",
        "        \"couldve\":\"could have\",\n",
        "        \"daren't\":\"dare not\",\n",
        "        \"daresn't\":\"dare not\",\n",
        "        \"dasn't\":\"dare not\",\n",
        "        \"didn't\":\"did not\",\n",
        "        \"didnt\":\"did not\",\n",
        "        \"didn\":\"did not\",\n",
        "        \"doesn't\":\"does not\",\n",
        "        \"doesnt\":\"does not\",\n",
        "        \"doesn\":\"does not\",\n",
        "        \"don't\":\"do not\",\n",
        "        \"dont\":\"do not\",\n",
        "        \"e'er\":\"ever\",\n",
        "        \"em\":\"them\",\n",
        "        \"everyone's\":\"everyone is\",\n",
        "        \"finna\":\"fixing to\",\n",
        "        \"gimme\":\"give me\",\n",
        "        \"gonna\":\"going to\",\n",
        "        \"gon't\":\"go not\",\n",
        "        \"gotta\":\"got to\",\n",
        "        \"hadn't\":\"had not\",\n",
        "        \"hadnt\":\"had not\",\n",
        "        \"hadn\":\"had not\",\n",
        "        \"hasn't\":\"has not\",\n",
        "        \"hasnt\":\"has not\",\n",
        "        \"hasn\":\"has not\",\n",
        "        \"haven't\":\"have not\",\n",
        "        \"havent\":\"have not\",\n",
        "        \"haven\":\"have not\",\n",
        "        \"he'd\":\"he would\",\n",
        "        \"he'll\":\"he will\",\n",
        "        \"he's\":\"he is\",\n",
        "        \"he've\":\"he have\",\n",
        "        \"how'd\":\"how would\",\n",
        "        \"how'll\":\"how will\",\n",
        "        \"how're\":\"how are\",\n",
        "        \"how's\":\"how is\",\n",
        "        \"I'd\":\"I would\",\n",
        "        \"I'll\":\"I will\",\n",
        "        \"I'm\":\"I am\",\n",
        "        \"I'm'a\":\"I am about to\",\n",
        "        \"I'm'o\":\"I am going to\",\n",
        "        \"isn't\":\"is not\",\n",
        "        \"it'd\":\"it would\",\n",
        "        \"it'll\":\"it will\",\n",
        "        \"it's\":\"it is\",\n",
        "        \"I've\":\"I have\",\n",
        "        \"kinda\":\"kind of\",\n",
        "        \"let's\":\"let us\",\n",
        "        \"mayn't\":\"may not\",\n",
        "        \"maynt\":\"may not\",\n",
        "        \"may've\":\"may have\",\n",
        "        \"mightn't\":\"might not\",\n",
        "        \"might've\":\"might have\",\n",
        "        \"mustn't\":\"must not\",\n",
        "        \"mustn't've\":\"must not have\",\n",
        "        \"must've\":\"must have\",\n",
        "        \"needn't\":\"need not\",\n",
        "        \"neednt\":\"need not\",\n",
        "        \"ne'er\":\"never\",\n",
        "        \"o'\":\"of\",\n",
        "        \"o'er\":\"over\",\n",
        "        \"ol'\":\"old\",\n",
        "        \"oughtn't\":\"ought not\",\n",
        "        \"shalln't\":\"shall not\",\n",
        "        \"shan't\":\"shall not\",\n",
        "        \"she'd\":\"she would\",\n",
        "        \"she'll\":\"she will\",\n",
        "        \"she's\":\"she is\",\n",
        "        \"shouldn't\":\"should not\",\n",
        "        \"shouldn't've\":\"should not have\",\n",
        "        \"should've\":\"should have\",\n",
        "        \"somebody's\":\"somebody is\",\n",
        "        \"someone's\":\"someone is\",\n",
        "        \"something's\":\"something is\",\n",
        "        \"that'd\":\"that would\",\n",
        "        \"that'll\":\"that will\",\n",
        "        \"that're\":\"that are\",\n",
        "        \"that's\":\"that is\",\n",
        "        \"there'd\":\"there would\",\n",
        "        \"there'll\":\"there will\",\n",
        "        \"there're\":\"there are\",\n",
        "        \"there's\":\"there is\",\n",
        "        \"these're\":\"these are\",\n",
        "        \"they'd\":\"they would\",\n",
        "        \"they'll\":\"they will\",\n",
        "        \"they're\":\"they are\",\n",
        "        \"they've\":\"they have\",\n",
        "        \"this's\":\"this is\",\n",
        "        \"those're\":\"those are\",\n",
        "        \"'tis\":\"it is\",\n",
        "        \"'twas\":\"it was\",\n",
        "        \"wanna\":\"want to\",\n",
        "        \"wasn't\":\"was not\",\n",
        "        \"we'd\":\"we would\",\n",
        "        \"we'd've\":\"we would have\",\n",
        "        \"we'll\":\"we will\",\n",
        "        \"we're\":\"we are\",\n",
        "        \"weren't\":\"were not\",\n",
        "        \"we've\":\"we have\",\n",
        "        \"what'd\":\"what did\",\n",
        "        \"what'll\":\"what will\",\n",
        "        \"what're\":\"what are\",\n",
        "        \"what's\":\"what is\",\n",
        "        \"what've\":\"what have\",\n",
        "        \"when's\":\"when is\",\n",
        "        \"where'd\":\"where did\",\n",
        "        \"where're\":\"where are\",\n",
        "        \"where's\":\"where is\",\n",
        "        \"where've\":\"where have\",\n",
        "        \"which's\":\"which is\",\n",
        "        \"who'd\":\"who would\",\n",
        "        \"who'd've\":\"who would have\",\n",
        "        \"who'll\":\"who will\",\n",
        "        \"who're\":\"who are\",\n",
        "        \"who's\":\"who is\",\n",
        "        \"who've\":\"who have\",\n",
        "        \"why'd\":\"why did\",\n",
        "        \"why're\":\"why are\",\n",
        "        \"why's\":\"why is\",\n",
        "        \"won't\":\"will not\",\n",
        "        \"wont\":\"will not\",\n",
        "        \"wouldn't\":\"would not\",\n",
        "        \"wouldnt\":\"would not\",\n",
        "        \"would've\":\"would have\",\n",
        "        \"y'all\":\"you all\",\n",
        "        \"you'd\":\"you would\",\n",
        "        \"you'll\":\"you will\",\n",
        "        \"you're\":\"you are\",\n",
        "        \"you've\":\"you have\",\n",
        "        \"Whatcha\":\"What are you\",\n",
        "        \"luv\":\"love\",\n",
        "        \"sux\":\"sucks\"\n",
        "        }"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKxpOZjRpONP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_dict_smileys():\n",
        "    \n",
        "    return {\n",
        "        \":‑)\":\"smiley\",\n",
        "        \":-]\":\"smiley\",\n",
        "        \":-3\":\"smiley\",\n",
        "        \":->\":\"smiley\",\n",
        "        \"8-)\":\"smiley\",\n",
        "        \":-}\":\"smiley\",\n",
        "        \":)\":\"smiley\",\n",
        "        \":]\":\"smiley\",\n",
        "        \":3\":\"smiley\",\n",
        "        \":>\":\"smiley\",\n",
        "        \"8)\":\"smiley\",\n",
        "        \":}\":\"smiley\",\n",
        "        \":o)\":\"smiley\",\n",
        "        \":c)\":\"smiley\",\n",
        "        \":^)\":\"smiley\",\n",
        "        \"=]\":\"smiley\",\n",
        "        \"=)\":\"smiley\",\n",
        "        \":-))\":\"smiley\",\n",
        "        \":‑D\":\"smiley\",\n",
        "        \"8‑D\":\"smiley\",\n",
        "        \"x‑D\":\"smiley\",\n",
        "        \"X‑D\":\"smiley\",\n",
        "        \":D\":\"smiley\",\n",
        "        \"8D\":\"smiley\",\n",
        "        \"xD\":\"smiley\",\n",
        "        \"XD\":\"smiley\",\n",
        "        \":‑(\":\"sad\",\n",
        "        \":‑c\":\"sad\",\n",
        "        \":‑<\":\"sad\",\n",
        "        \":‑[\":\"sad\",\n",
        "        \":(\":\"sad\",\n",
        "        \":c\":\"sad\",\n",
        "        \":<\":\"sad\",\n",
        "        \":[\":\"sad\",\n",
        "        \":-||\":\"sad\",\n",
        "        \">:[\":\"sad\",\n",
        "        \":{\":\"sad\",\n",
        "        \":@\":\"sad\",\n",
        "        \">:(\":\"sad\",\n",
        "        \":'‑(\":\"sad\",\n",
        "        \":'(\":\"sad\",\n",
        "        \":‑P\":\"playful\",\n",
        "        \"X‑P\":\"playful\",\n",
        "        \"x‑p\":\"playful\",\n",
        "        \":‑p\":\"playful\",\n",
        "        \":‑Þ\":\"playful\",\n",
        "        \":‑þ\":\"playful\",\n",
        "        \":‑b\":\"playful\",\n",
        "        \":P\":\"playful\",\n",
        "        \"XP\":\"playful\",\n",
        "        \"xp\":\"playful\",\n",
        "        \":p\":\"playful\",\n",
        "        \":Þ\":\"playful\",\n",
        "        \":þ\":\"playful\",\n",
        "        \":b\":\"playful\",\n",
        "        \"<3\":\"love\"\n",
        "        }"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NHqm8EJENPEE",
        "colab": {}
      },
      "source": [
        "def clean_text_r1(text):\n",
        "    text= text.lower()\n",
        "    \n",
        "    text = re.sub(\"(@[A-Za-z0-9]+)|(#[A-Za-z0-9]+)\", \" \", text)  #remove mentions and hashtag\n",
        "    text= re.sub('\\[.*?\\]','',text) #remove punctuations\n",
        "    #print(text)\n",
        "    text =re.sub(\"(\\w+:\\/\\/\\S+)\", \" \", text) ##remove url\n",
        "    #print(text)\n",
        "\n",
        "    text = re.sub('\\w*\\d','',text) #remove digits after words or in between words\n",
        "    #print(text)\n",
        "    CONTRACTIONS = load_dict_contractions()\n",
        "    text = text.replace(\"’\",\"'\")\n",
        "    words = text.split()\n",
        "    reformed = [CONTRACTIONS[word] if word in CONTRACTIONS else word for word in words]\n",
        "    text = \" \".join(reformed)\n",
        "    \n",
        "    text = ''.join(''.join(s)[:2] for _, s in itertools.groupby(text))\n",
        "    #emojis\n",
        "    SMILEY = load_dict_smileys()  \n",
        "    words = text.split()\n",
        "    reformed = [SMILEY[word] if word in SMILEY else word for word in words]\n",
        "    text = \" \".join(reformed)\n",
        "    #print(text)\n",
        "    text = emoji.demojize(text)\n",
        "    text = text.replace(\":\",\" \")\n",
        "    text = ' '.join(text.split())\n",
        "    #print(text)\n",
        "    text= re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
        "    #print(text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def remove_one_or_two_alpha(tweet):  \n",
        "  for word in tweet:\n",
        "    if(len(word) < 3):\n",
        "      tweet.remove(word)\n",
        "  return tweet\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Tu63rj48NPEG",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "9375d53d-b855-4fb1-fd15-79750126e47e"
      },
      "source": [
        "\n",
        "# Text Preprocessing\n",
        "def cleaning_part2(dataset):\n",
        "    wordNetLemmatizer = WordNetLemmatizer()\n",
        "    porterStemmer = PorterStemmer()\n",
        "    data_clean = dataset.apply(lambda x: clean_text_r1(x))\n",
        "\n",
        "    data_clean =data_clean.apply(lambda tweet: [wordNetLemmatizer.lemmatize(word) for word in nltk.word_tokenize(tweet)])\n",
        "    data_clean = data_clean.apply(lambda tweet: [word for word in tweet if word not in stopwords.words('english')])\n",
        "    data_clean = data_clean.apply(lambda tweet : remove_one_or_two_alpha(tweet))\n",
        "    data_clean = data_clean.apply(lambda tweet: ' '.join(tweet))\n",
        "    data_clean_list =[]\n",
        "    for tweet in data_clean:\n",
        "        data_clean_list.append(tweet);\n",
        "    return data_clean_list\n",
        "\n",
        "def label(senti):\n",
        "  if(senti == \"negative\"):\n",
        "    senti =0\n",
        "  if(senti == \"neutral\"):\n",
        "    senti= 1\n",
        "  if(senti == \"positive\"):\n",
        "    senti = 2\n",
        "  return senti\n",
        "\n",
        "reviews_train_clean = cleaning_part2(train_dataset['tweet_text'])\n",
        "reviews_labels =[]\n",
        "for sentiment in train_dataset['sentiment']:\n",
        "  sentiment = label(sentiment)\n",
        "  reviews_labels.append(sentiment)\n",
        "\n",
        "#uncomment it if you want to Check cleaning of texts by saving them into another file.  \n",
        "'''\n",
        "# Create DataFrame \n",
        "dfn = pd.DataFrame()\n",
        "dfn['tweet_orig'] = train_dataset['tweet_text']\n",
        "dfn['tweet_new'] = reviews_train_clean\n",
        "\n",
        "dfn.to_csv('/content/drive/My Drive/lrtext1.csv', header=True, index=False) \n",
        "\n",
        "\n",
        "reviews_train_clean = cleaningPart2(train_dataset['tweet_text'])\n",
        "  '''\n",
        "    \n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n# Create DataFrame \\ndfn = pd.DataFrame()\\ndfn['tweet_orig'] = train_dataset['tweet_text']\\ndfn['tweet_new'] = reviews_train_clean\\n\\ndfn.to_csv('/content/drive/My Drive/lrtext1.csv', header=True, index=False) \\n\\n\\nreviews_train_clean = cleaningPart2(train_dataset['tweet_text'])\\n  \""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwO1eM80pONk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 890
        },
        "outputId": "d16b27b2-ca58-4773-d9cd-868c06bcb299"
      },
      "source": [
        "import os\n",
        "import tensorflow\n",
        "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import*\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.regularizers import L1L2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import spatial\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "from keras.initializers import Constant\n",
        "from termcolor import colored\n",
        "\n",
        "max_features = 19000\n",
        "\n",
        "# Tokenization\n",
        "print(colored(\"Tokenizing and padding data\", \"yellow\"))\n",
        "tokenizer = Tokenizer(num_words = max_features, split = ' ', oov_token='<unw>')\n",
        "tokenizer.fit_on_texts(reviews_train_clean)\n",
        "train_tweets = tokenizer.texts_to_sequences(reviews_train_clean)\n",
        "max_len = max([len(i) for i in train_tweets])\n",
        "train_tweets = pad_sequences(train_tweets, maxlen = max_len)\n",
        "\n",
        "print(colored(\"Tokenizing and padding complete\", \"yellow\"))\n",
        "\n",
        "## embedding code \n",
        "embeddings_index = {}\n",
        "f = open(os.path.join('/content/drive/My Drive/', 'glove.twitter.27B.100d.txt'))\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "num_words = min(max_features, len(word_index)) + 1\n",
        "print(num_words)\n",
        "\n",
        "embedding_dim = 100\n",
        "\n",
        "# first create a matrix of zeros, this is our embedding matrix\n",
        "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
        "\n",
        "# for each word in out tokenizer lets try to find that work in our w2v model\n",
        "for word, i in word_index.items():\n",
        "    if i > max_features:\n",
        "        continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # we found the word - add that words vector to the matrix\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    else:\n",
        "        # doesn't exist, assign a random vector\n",
        "        embedding_matrix[i] = np.random.randn(embedding_dim)\n",
        "\n",
        "\n",
        "# Building the model\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(num_words,\n",
        "                    embedding_dim,\n",
        "                    embeddings_initializer=Constant(embedding_matrix),\n",
        "                    input_length = train_tweets.shape[1],\n",
        "                    trainable=True))\n",
        "\n",
        "model.add(SpatialDropout1D(0.4))\n",
        "model.add(LSTM(100,dropout = 0.3, recurrent_dropout=0.2))\n",
        "model.add(Dense(3, init='uniform'))\n",
        "model.add(BatchNormalization(epsilon=1e-06, mode=0, momentum=0.9, weights=None))\n",
        "model.add(Activation('softmax'))\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# Training the model\n",
        "print(colored(\"Training the LSTM model\", \"green\"))\n",
        "history = model.fit(train_tweets, pd.get_dummies(train_dataset['sentiment']).values, epochs = 8, batch_size = 128, validation_split = 0.2)\n",
        "print(colored(history, \"green\"))\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mTokenizing and padding data\u001b[0m\n",
            "\u001b[33mTokenizing and padding complete\u001b[0m\n",
            "Found 1193514 word vectors.\n",
            "Found 21990 unique tokens.\n",
            "19001\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:77: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(3, kernel_initializer=\"uniform\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:78: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(epsilon=1e-06, momentum=0.9, weights=None)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 530, 100)          1900100   \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d_1 (Spatial (None, 530, 100)          0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 100)               80400     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 3)                 303       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 3)                 0         \n",
            "=================================================================\n",
            "Total params: 1,980,815\n",
            "Trainable params: 1,980,809\n",
            "Non-trainable params: 6\n",
            "_________________________________________________________________\n",
            "\u001b[32mTraining the LSTM model\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 17172 samples, validate on 4293 samples\n",
            "Epoch 1/8\n",
            "17172/17172 [==============================] - 135s 8ms/step - loss: 1.0292 - accuracy: 0.4902 - val_loss: 0.9569 - val_accuracy: 0.5295\n",
            "Epoch 2/8\n",
            "17172/17172 [==============================] - 134s 8ms/step - loss: 0.9224 - accuracy: 0.5701 - val_loss: 0.9117 - val_accuracy: 0.5570\n",
            "Epoch 3/8\n",
            "17172/17172 [==============================] - 134s 8ms/step - loss: 0.8596 - accuracy: 0.6090 - val_loss: 0.9029 - val_accuracy: 0.5770\n",
            "Epoch 4/8\n",
            "17172/17172 [==============================] - 133s 8ms/step - loss: 0.8112 - accuracy: 0.6377 - val_loss: 0.8932 - val_accuracy: 0.5798\n",
            "Epoch 5/8\n",
            "17172/17172 [==============================] - 133s 8ms/step - loss: 0.7722 - accuracy: 0.6575 - val_loss: 0.8901 - val_accuracy: 0.5903\n",
            "Epoch 6/8\n",
            "17172/17172 [==============================] - 133s 8ms/step - loss: 0.7427 - accuracy: 0.6746 - val_loss: 0.9002 - val_accuracy: 0.5851\n",
            "Epoch 7/8\n",
            "17172/17172 [==============================] - 132s 8ms/step - loss: 0.7095 - accuracy: 0.6936 - val_loss: 0.8906 - val_accuracy: 0.5952\n",
            "Epoch 8/8\n",
            "17172/17172 [==============================] - 132s 8ms/step - loss: 0.6805 - accuracy: 0.7114 - val_loss: 0.8862 - val_accuracy: 0.5961\n",
            "\u001b[32m<keras.callbacks.callbacks.History object at 0x7f7be17c4be0>\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycjvKhx6pONn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "5aa1a205-23cb-4601-ff3d-42f4a8213ddb"
      },
      "source": [
        "# Load data\n",
        "test = pd.read_csv('/content/drive/My Drive/test_samples.txt')\n",
        "test_data = pd.DataFrame(test, columns= ['tweet_id','tweet_text'])\n",
        "td = cleaning_part2(test_data['tweet_text'])\n",
        "max_len = max([len(i) for i in train_tweets])\n",
        "print(max_len,\"maxlen\")\n",
        "print(colored(\"Test Data loaded\", \"yellow\"))\n",
        "\n",
        "test_tweets = tokenizer.texts_to_sequences(td)\n",
        "test_tweets = pad_sequences(test_tweets, maxlen = max_len)\n",
        "\n",
        "result = model.predict_classes(test_tweets)\n",
        "labels=[]\n",
        "for res in result:\n",
        "  if(res==0):\n",
        "    labels.append(\"negative\")\n",
        "  if(res==1):\n",
        "    labels.append(\"neutral\")\n",
        "  if(res==2):\n",
        "    labels.append(\"positive\")\n",
        "\n",
        "ids = []\n",
        "test_data['tweet_id'].apply(lambda x : ids.append(x))\n",
        "data = {'tweet_id':ids , 'sentiment':labels} \n",
        "# Create DataFrame \n",
        "df = pd.DataFrame() \n",
        "df['tweet_id'] = ids\n",
        "df['sentiment'] = labels\n",
        "print(df)\n",
        "df.to_csv('/content/drive/My Drive/test_accuracy2.csv', header=True, index=False)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "530 maxlen\n",
            "\u001b[33mTest Data loaded\u001b[0m\n",
            "                tweet_id sentiment\n",
            "0     264238274963451904   neutral\n",
            "1     218775148495515649  positive\n",
            "2     258965201766998017   neutral\n",
            "3     262926411352903682   neutral\n",
            "4     171874368908050432  negative\n",
            "...                  ...       ...\n",
            "5393  210378118865756160   neutral\n",
            "5394  245177521304399872  positive\n",
            "5395  259280987089932288   neutral\n",
            "5396  201113950211940352   neutral\n",
            "5397  237999067286876160   neutral\n",
            "\n",
            "[5398 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}